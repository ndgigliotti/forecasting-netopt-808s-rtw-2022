{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research list:\n",
    "- Handling outliers in timeseries forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "## Importing Libraries\n",
    "import sys\n",
    "import numbers\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from functools import reduce\n",
    "\n",
    "import pmdarima as pmd\n",
    "import statsmodels.api as sm \n",
    "from scipy.stats import normaltest\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import (\n",
    "    NaiveSeasonal,\n",
    "    NaiveDrift,\n",
    "    Prophet,\n",
    "    ExponentialSmoothing,\n",
    "    ARIMA,\n",
    "    AutoARIMA,\n",
    "    Theta, \n",
    "    RegressionEnsembleModel)               # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "from darts.metrics import mape, mase, mae, mse, ope, r2_score, rmse, rmsle\n",
    "from darts.utils.statistics import check_seasonality, plot_acf, plot_residuals_analysis\n",
    "from darts.dataprocessing.transformers.boxcox import BoxCox\n",
    "from darts.utils.utils import ModelMode           # new \n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "\n",
    "from darts.datasets import ( \n",
    "    AirPassengersDataset, AusBeerDataset, GasRateCO2Dataset, HeartRateDataset, \n",
    "    IceCreamHeaterDataset, MonthlyMilkDataset, SunspotsDataset)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "\n",
    "\n",
    "TRACE = False                 # print also the suboptimal models while SARIMA tuning process is running\n",
    "MSEAS = 12                    # seasonality default\n",
    "ALPHA = 0.1                  # significance level default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Resources/RK_RFCU_Gross_Chargeoffs.csv?'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## load data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m mdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Resources/RK_RFCU_Gross_Chargeoffs.csv?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMONTH_END_DT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMONTH_END_DT\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(mdf\u001b[38;5;241m.\u001b[39minfo())\n",
      "File \u001b[0;32m~/Environments/base38/lib64/python3.8/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Environments/base38/lib64/python3.8/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Environments/base38/lib64/python3.8/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Environments/base38/lib64/python3.8/site-packages/pandas/io/parsers/readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Environments/base38/lib64/python3.8/site-packages/pandas/io/parsers/readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Environments/base38/lib64/python3.8/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Resources/RK_RFCU_Gross_Chargeoffs.csv?'"
     ]
    }
   ],
   "source": [
    "## load data\n",
    "\n",
    "mdf = pd.read_csv('./Resources/RK_RFCU_Gross_Chargeoffs.csv?')\n",
    "mdf['MONTH_END_DT'] = pd.to_datetime(mdf['MONTH_END_DT'])\n",
    "print(mdf.info())\n",
    "\n",
    "#Create a DF of only RFCU data\n",
    "rfcu_df = mdf[['MONTH_END_DT', 'RFCU_CC_Chargeoff']]\n",
    "rfcu_df = rfcu_df[rfcu_df['RFCU_CC_Chargeoff'] != 0]\n",
    "# rfcu_df.set_index(keys='MONTH_END_DT', inplace=True, drop=True)\n",
    "\n",
    "#Create a DF of only RK data\n",
    "rk_df = mdf[['MONTH_END_DT', 'RFCU_CC_Chargeoff']]\n",
    "rk_df = rk_df[rk_df['RFCU_CC_Chargeoff'] != 0]\n",
    "# rk_df.set_index(keys='MONTH_END_DT', inplace=True, drop=True)\n",
    "\n",
    "# Select which df you want to put into the timeseries\n",
    "# series = rk_df\n",
    "series = rfcu_df\n",
    "mdf = rfcu_df\n",
    "mdf.columns = ['month', 'chargeoff_amt']\n",
    "series.columns = ['month', 'chargeoff_amt']\n",
    "series.set_index(keys='month', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to snowflake\n",
    "from snowflake.snowpark import Session\n",
    "import json\n",
    "credentials = json.load(open('secrets.json'))[\"SNOWFLAKE_CONNECTION\"]\n",
    "\n",
    "session = Session.builder.configs(credentials).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_database(\"Covid19\")\n",
    "session.use_schema(\"public\")\n",
    "mdf = session.table(\"CDC_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SnowparkFetchDataException",
     "evalue": "(1406): Failed to fetch a Pandas Dataframe. The error is: 255002: Optional dependency: 'pandas' is not installed, please see the following link for install instructions: https://docs.snowflake.com/en/user-guide/python-connector-pandas.html#installation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:386\u001b[0m, in \u001b[0;36mServerConnection._to_data_or_iter\u001b[1;34m(self, results_cursor, to_pandas, to_iter)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    377\u001b[0m     data_or_iter \u001b[39m=\u001b[39m (\n\u001b[0;32m    378\u001b[0m         \u001b[39mmap\u001b[39m(\n\u001b[0;32m    379\u001b[0m             functools\u001b[39m.\u001b[39mpartial(\n\u001b[0;32m    380\u001b[0m                 _fix_pandas_df_integer, results_cursor\u001b[39m=\u001b[39mresults_cursor\n\u001b[0;32m    381\u001b[0m             ),\n\u001b[0;32m    382\u001b[0m             results_cursor\u001b[39m.\u001b[39mfetch_pandas_batches(),\n\u001b[0;32m    383\u001b[0m         )\n\u001b[0;32m    384\u001b[0m         \u001b[39mif\u001b[39;00m to_iter\n\u001b[0;32m    385\u001b[0m         \u001b[39melse\u001b[39;00m _fix_pandas_df_integer(\n\u001b[1;32m--> 386\u001b[0m             results_cursor\u001b[39m.\u001b[39;49mfetch_pandas_all(), results_cursor\n\u001b[0;32m    387\u001b[0m         )\n\u001b[0;32m    388\u001b[0m     )\n\u001b[0;32m    389\u001b[0m \u001b[39mexcept\u001b[39;00m NotSupportedError:\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\connector\\cursor.py:986\u001b[0m, in \u001b[0;36mSnowflakeCursor.fetch_pandas_all\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[39m\"\"\"Fetch Pandas dataframes in batches, where 'batch' refers to Snowflake Chunk.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_can_use_pandas()\n\u001b[0;32m    987\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prefetch_hook \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\connector\\cursor.py:910\u001b[0m, in \u001b[0;36mSnowflakeCursor.check_can_use_pandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    908\u001b[0m errno \u001b[39m=\u001b[39m ER_NO_PYARROW\n\u001b[1;32m--> 910\u001b[0m Error\u001b[39m.\u001b[39;49merrorhandler_wrapper(\n\u001b[0;32m    911\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection,\n\u001b[0;32m    912\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    913\u001b[0m     ProgrammingError,\n\u001b[0;32m    914\u001b[0m     {\n\u001b[0;32m    915\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmsg\u001b[39;49m\u001b[39m\"\u001b[39;49m: msg,\n\u001b[0;32m    916\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39merrno\u001b[39;49m\u001b[39m\"\u001b[39;49m: errno,\n\u001b[0;32m    917\u001b[0m     },\n\u001b[0;32m    918\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\connector\\errors.py:275\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \n\u001b[0;32m    261\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[39m    exception to the first handler in that order.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m handed_over \u001b[39m=\u001b[39m Error\u001b[39m.\u001b[39;49mhand_to_other_handler(\n\u001b[0;32m    276\u001b[0m     connection,\n\u001b[0;32m    277\u001b[0m     cursor,\n\u001b[0;32m    278\u001b[0m     error_class,\n\u001b[0;32m    279\u001b[0m     error_value,\n\u001b[0;32m    280\u001b[0m )\n\u001b[0;32m    281\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m handed_over:\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\connector\\errors.py:330\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    329\u001b[0m cursor\u001b[39m.\u001b[39mmessages\u001b[39m.\u001b[39mappend((error_class, error_value))\n\u001b[1;32m--> 330\u001b[0m cursor\u001b[39m.\u001b[39;49merrorhandler(connection, cursor, error_class, error_value)\n\u001b[0;32m    331\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\connector\\errors.py:209\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[1;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m\"\"\"Default error handler that raises an error.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \n\u001b[0;32m    200\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39m    A Snowflake error.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m \u001b[39mraise\u001b[39;00m error_class(\n\u001b[0;32m    210\u001b[0m     msg\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmsg\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    211\u001b[0m     errno\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merrno\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    212\u001b[0m     sqlstate\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msqlstate\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    213\u001b[0m     sfqid\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    214\u001b[0m     done_format_msg\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdone_format_msg\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    215\u001b[0m     connection\u001b[39m=\u001b[39mconnection,\n\u001b[0;32m    216\u001b[0m     cursor\u001b[39m=\u001b[39mcursor,\n\u001b[0;32m    217\u001b[0m )\n",
      "\u001b[1;31mProgrammingError\u001b[0m: 255002: Optional dependency: 'pandas' is not installed, please see the following link for install instructions: https://docs.snowflake.com/en/user-guide/python-connector-pandas.html#installation",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSnowparkFetchDataException\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\telemetry.py:138\u001b[0m, in \u001b[0;36mdf_collect_api_telemetry.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    137\u001b[0m     \u001b[39mwith\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_session\u001b[39m.\u001b[39mquery_history() \u001b[39mas\u001b[39;00m query_history:\n\u001b[1;32m--> 138\u001b[0m         result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    139\u001b[0m     plan \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_select_statement \u001b[39mor\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_plan\n\u001b[0;32m    140\u001b[0m     api_calls \u001b[39m=\u001b[39m [\n\u001b[0;32m    141\u001b[0m         \u001b[39m*\u001b[39mplan\u001b[39m.\u001b[39mapi_calls,\n\u001b[0;32m    142\u001b[0m         {TelemetryField\u001b[39m.\u001b[39mNAME\u001b[39m.\u001b[39mvalue: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataFrame.\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    143\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\snowpark\\dataframe.py:728\u001b[0m, in \u001b[0;36mDataFrame.to_pandas\u001b[1;34m(self, statement_params, block, **kwargs)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    723\u001b[0m     warning(\n\u001b[0;32m    724\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto_pandas.block\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    725\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mblock argument is experimental. Do not use it in production.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    726\u001b[0m     )\n\u001b[1;32m--> 728\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    729\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plan,\n\u001b[0;32m    730\u001b[0m     to_pandas\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    731\u001b[0m     block\u001b[39m=\u001b[39;49mblock,\n\u001b[0;32m    732\u001b[0m     data_type\u001b[39m=\u001b[39;49m_AsyncResultType\u001b[39m.\u001b[39;49mPANDAS,\n\u001b[0;32m    733\u001b[0m     _statement_params\u001b[39m=\u001b[39;49mcreate_or_update_statement_params_with_query_tag(\n\u001b[0;32m    734\u001b[0m         statement_params, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49mquery_tag, SKIP_LEVELS_TWO\n\u001b[0;32m    735\u001b[0m     ),\n\u001b[0;32m    736\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    737\u001b[0m )\n\u001b[0;32m    739\u001b[0m \u001b[39m# if the returned result is not a pandas dataframe, raise Exception\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[39m# this might happen when calling this method with non-select commands\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \u001b[39m# e.g., session.sql(\"create ...\").to_pandas()\u001b[39;00m\n\u001b[0;32m    742\u001b[0m \u001b[39mif\u001b[39;00m block:\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:421\u001b[0m, in \u001b[0;36mServerConnection.execute\u001b[1;34m(self, plan, to_pandas, to_iter, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mif\u001b[39;00m is_in_stored_procedure() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    418\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    419\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsync query is not supported in stored procedure yet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m     )\n\u001b[1;32m--> 421\u001b[0m result_set, result_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_result_set(\n\u001b[0;32m    422\u001b[0m     plan, to_pandas, to_iter, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, block\u001b[39m=\u001b[39;49mblock, data_type\u001b[39m=\u001b[39;49mdata_type\n\u001b[0;32m    423\u001b[0m )\n\u001b[0;32m    424\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m result_set\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\analyzer\\snowflake_plan.py:85\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     84\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     86\u001b[0m     \u001b[39mexcept\u001b[39;00m snowflake\u001b[39m.\u001b[39mconnector\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mProgrammingError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     87\u001b[0m         tb \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:509\u001b[0m, in \u001b[0;36mServerConnection.get_result_set\u001b[1;34m(self, plan, to_pandas, to_iter, block, data_type, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39mfor\u001b[39;00m holder, id_ \u001b[39min\u001b[39;00m placeholders\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    508\u001b[0m     final_query \u001b[39m=\u001b[39m final_query\u001b[39m.\u001b[39mreplace(holder, id_)\n\u001b[1;32m--> 509\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_query(\n\u001b[0;32m    510\u001b[0m     final_query,\n\u001b[0;32m    511\u001b[0m     to_pandas,\n\u001b[0;32m    512\u001b[0m     to_iter \u001b[39mand\u001b[39;49;00m (i \u001b[39m==\u001b[39;49m \u001b[39mlen\u001b[39;49m(plan\u001b[39m.\u001b[39;49mqueries) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m),\n\u001b[0;32m    513\u001b[0m     is_ddl_on_temp_object\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mis_ddl_on_temp_object,\n\u001b[0;32m    514\u001b[0m     block\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_last,\n\u001b[0;32m    515\u001b[0m     data_type\u001b[39m=\u001b[39;49mdata_type,\n\u001b[0;32m    516\u001b[0m     async_job_plan\u001b[39m=\u001b[39;49mplan,\n\u001b[0;32m    517\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    518\u001b[0m )\n\u001b[0;32m    519\u001b[0m placeholders[query\u001b[39m.\u001b[39mquery_id_place_holder] \u001b[39m=\u001b[39m (\n\u001b[0;32m    520\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_last \u001b[39melse\u001b[39;00m result\u001b[39m.\u001b[39mquery_id\n\u001b[0;32m    521\u001b[0m )\n\u001b[0;32m    522\u001b[0m result_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cursor\u001b[39m.\u001b[39mdescription\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:105\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m    102\u001b[0m         ex\u001b[39m.\u001b[39mcause\n\u001b[0;32m    103\u001b[0m     )\n\u001b[0;32m    104\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mraise\u001b[39;00m ex\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:99\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[0;32m     98\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    100\u001b[0m \u001b[39mexcept\u001b[39;00m ReauthenticationRequest \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m    101\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[0;32m    102\u001b[0m         ex\u001b[39m.\u001b[39mcause\n\u001b[0;32m    103\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:356\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[1;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[39m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[39m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[39m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[39m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[39m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m--> 356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_data_or_iter(\n\u001b[0;32m    357\u001b[0m         results_cursor\u001b[39m=\u001b[39;49mresults_cursor, to_pandas\u001b[39m=\u001b[39;49mto_pandas, to_iter\u001b[39m=\u001b[39;49mto_iter\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    359\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m AsyncJob(\n\u001b[0;32m    361\u001b[0m         results_cursor[\u001b[39m\"\u001b[39m\u001b[39mqueryId\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    362\u001b[0m         query,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    367\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\joebu\\programming_directory\\hackathon\\venv\\lib\\site-packages\\snowflake\\snowpark\\_internal\\server_connection.py:396\u001b[0m, in \u001b[0;36mServerConnection._to_data_or_iter\u001b[1;34m(self, results_cursor, to_pandas, to_iter)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m--> 396\u001b[0m         \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_FAILED_FETCH_PANDAS(\n\u001b[0;32m    397\u001b[0m             \u001b[39mstr\u001b[39m(ex)\n\u001b[0;32m    398\u001b[0m         )\n\u001b[0;32m    399\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    400\u001b[0m     data_or_iter \u001b[39m=\u001b[39m (\n\u001b[0;32m    401\u001b[0m         \u001b[39miter\u001b[39m(results_cursor) \u001b[39mif\u001b[39;00m to_iter \u001b[39melse\u001b[39;00m results_cursor\u001b[39m.\u001b[39mfetchall()\n\u001b[0;32m    402\u001b[0m     )\n",
      "\u001b[1;31mSnowparkFetchDataException\u001b[0m: (1406): Failed to fetch a Pandas Dataframe. The error is: 255002: Optional dependency: 'pandas' is not installed, please see the following link for install instructions: https://docs.snowflake.com/en/user-guide/python-connector-pandas.html#installation"
     ]
    }
   ],
   "source": [
    "mdf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the observations\n",
    "\n",
    "plt.figure(100, figsize=(12, 5))\n",
    "series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Outliers considering seasonality and trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers despite seasonality and trend\n",
    "#compute seasonal decomposition\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomp = seasonal_decompose(series, period=12)\n",
    "\n",
    "#plot decomp\n",
    "fig = decomp.plot()\n",
    "fig.set_size_inches((16, 11))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add decomp series to the df\n",
    "\n",
    "series['trend'] = decomp.trend\n",
    "series['seasonal'] = decomp.seasonal\n",
    "series['resid'] = decomp.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define 90% upper and lower bounds\n",
    "\n",
    "z_val = 3\n",
    "series['resid_UB'] = series['resid'].mean() + z_val * series['resid'].std()\n",
    "series['resid_LB'] = series['resid'].mean() - z_val * series['resid'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add anomaly flags\n",
    "\n",
    "series['is_anomaly'] = (np.where((series['resid'] > series['resid_UB'])\n",
    "                             | (series['resid'] < series['resid_LB']),\n",
    "                             1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display anomalies\n",
    "display(series[series['is_anomaly'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the anamolies, and impute with darts\n",
    "series = series[series['is_anomaly'] != 1]\n",
    "\n",
    "ts_series = series['chargeoff_amt']\n",
    "ts = TimeSeries.from_series(ts_series, fill_missing_dates=True)\n",
    "ts = fill_missing_values(ts, fill='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the new time series\n",
    "plt.figure(100, figsize=(12, 5))\n",
    "ts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and check ACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF to check for autocorrelation\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "plot_acf(mdf.chargeoff_amt)\n",
    "result = adfuller(rfcu_df.chargeoff_amt.dropna())\n",
    "print('p-value: ', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First order differencing\n",
    "f, axs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "axs[0] = f.add_subplot(121)\n",
    "axs[0].set_title('1st Order Differencing')\n",
    "axs[0].plot(mdf.chargeoff_amt.diff())\n",
    "#Plot PCAF and check it\n",
    "\n",
    "\n",
    "plot_acf(mdf.chargeoff_amt.diff().dropna(), ax=axs[1])\n",
    "plt.show()\n",
    "\n",
    "#Test statistic\n",
    "result = adfuller(mdf.chargeoff_amt.diff().dropna())\n",
    "print('p-value: ', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second order differencing\n",
    "f, axs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "axs[0].set_title('2nd Order Differencing')\n",
    "axs[0].plot(mdf.chargeoff_amt.diff().diff())\n",
    "\n",
    "\n",
    "plot_acf(mdf.chargeoff_amt.diff().diff().dropna(), ax=axs[1])\n",
    "plt.show()\n",
    "result = adfuller(mdf.chargeoff_amt.diff().diff().dropna())\n",
    "print('p-value: ', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for seasonality. mseas will be used as an input to models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for seasonality, via ACF\n",
    "\n",
    "for m in range(2, 25):\n",
    "    is_seasonal, mseas = check_seasonality(ts, m=m, alpha=ALPHA)\n",
    "    if is_seasonal:\n",
    "        break\n",
    "    else:\n",
    "        mseas = 12\n",
    "\n",
    "print(\"seasonal? \" + str(is_seasonal))\n",
    "if is_seasonal:\n",
    "    print('There is seasonality of order {}.'.format(mseas))\n",
    "print(f'mseas = {mseas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split train and test data\n",
    "\n",
    "# split position: if string, then interpret as Timestamp\n",
    "# if int, then interpretation as index\n",
    "# if loat, then interpretation as %split\n",
    "\n",
    "# if isinstance(TRAIN, numbers.Number):\n",
    "#     split_at = TRAIN\n",
    "# else:\n",
    "#     split_at = pd.Timestamp(TRAIN)\n",
    "# train, val = ts.split_before(split_at)\n",
    "\n",
    "from darts.utils import model_selection\n",
    "# train, val = model_selection.train_test_split(ts, test_size=0.33, axis=0, input_size=0, horizon=0, vertical_split_type='simple', lazy=False)\n",
    "train, val = model_selection.train_test_split(ts, test_size=0.25, axis=0, vertical_split_type='model-aware', horizon=3, input_size=3, lazy=False)\n",
    "\n",
    "plt.figure(101, figsize=(12, 5))\n",
    "train.plot(label='training')\n",
    "val.plot(label='validation')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for pipeline. Building Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy metrics and processing time\n",
    "\n",
    "def accuracy_metrics(act, forecast, resid, t_start):\n",
    "    sr = resid.pd_series()\n",
    "    sa = act.pd_series()\n",
    "    n_act = len(act)\n",
    "    res_mape = mape(act, forecast)\n",
    "    res_mae = mae(act, forecast)\n",
    "    res_r2 = r2_score(act, forecast)\n",
    "    res_rmse = rmse(act, forecast)\n",
    "    res_rmsle = rmsle(act, forecast)\n",
    "    res_pe = sr / sa\n",
    "    res_rmspe = np.sqrt(np.sum(res_pe**2) / n_act)    # root mean square percentage error\n",
    "\n",
    "    res_time = time.perf_counter() - t_start\n",
    "    \n",
    "    res_mean = np.mean(sr)\n",
    "    res_std = np.std(sr)                               # std error of the model = std deviation of the noise\n",
    "    res_se = res_std / np.sqrt(n_act)                  # std error in estimating the mean\n",
    "    res_sefc = np.sqrt(res_std + res_se**2)            # std error of the forecast\n",
    "    \n",
    "    res_accuracy = {\"MAPE\":res_mape,\"RMSE\":res_rmse, \"-R squared\":-res_r2, \n",
    "        \"se\": res_sefc, \"time\":res_time}\n",
    "    return res_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chance to set mseas manually to check model performance. Comment out to use mseas from above\n",
    "# mseas = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the chosen forecaster model and compute predictions\n",
    "\n",
    "def eval_model(model):\n",
    "    t_start =  time.perf_counter()\n",
    "    strmodel = str(model)[:30]\n",
    "    print(\"beginning: \" + strmodel)\n",
    "\n",
    "\n",
    "    # fit the model and compute predictions\n",
    "    n_val = len(val)\n",
    "    res = model.fit(train)\n",
    "    forecast = model.predict(n_val)\n",
    "\n",
    "\n",
    "    # for naive forecast, concatenate seasonal fc with drift fc\n",
    "    if model == m_naive:\n",
    "        if is_seasonal:\n",
    "            fc_drift = forecast\n",
    "            modelS = NaiveSeasonal(K=mseas)\n",
    "            modelS.fit(train)\n",
    "            fc_seas = modelS.predict(len(val))\n",
    "            forecast = fc_drift + fc_seas - train.last_value()\n",
    "\n",
    "\n",
    "    resid = forecast - val\n",
    "    res_accuracy = accuracy_metrics(val, forecast, resid, t_start)\n",
    "    \n",
    "    \n",
    "    results = [forecast, res_accuracy]\n",
    "    \n",
    "    print(\"completed: \" + strmodel + \":\" + str(res_accuracy[\"time\"]) + \" sec\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: Naive drift model\n"
     ]
    }
   ],
   "source": [
    "# prepare Naive forecaster\n",
    "\n",
    "m_naive = NaiveDrift()\n",
    "print(\"model:\", m_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_seasonal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m exp_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmult\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Do research here to understand more about using exponential smoothing forecaster when data is seasonal.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_seasonal\u001b[49m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exp_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     10\u001b[0m         m_expon \u001b[38;5;241m=\u001b[39m ExponentialSmoothing( trend\u001b[38;5;241m=\u001b[39mModelMode\u001b[38;5;241m.\u001b[39mADDITIVE, \n\u001b[1;32m     11\u001b[0m                                     damped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     12\u001b[0m                                     seasonal\u001b[38;5;241m=\u001b[39mModelMode\u001b[38;5;241m.\u001b[39mADDITIVE, \n\u001b[1;32m     13\u001b[0m                                     seasonal_periods\u001b[38;5;241m=\u001b[39mmseas) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_seasonal' is not defined"
     ]
    }
   ],
   "source": [
    "# prepare Exponential Smoothing forecaster\n",
    "\n",
    "# SET EXPONENTIAL SMOOTHING MODE:\n",
    "# exp_mode = 'add'\n",
    "exp_mode = 'mult'\n",
    "\n",
    "# Do research here to understand more about using exponential smoothing forecaster when data is seasonal.\n",
    "if is_seasonal:\n",
    "    if exp_mode == 'add':\n",
    "        m_expon = ExponentialSmoothing( trend=ModelMode.ADDITIVE, \n",
    "                                    damped=False, \n",
    "                                    seasonal=ModelMode.ADDITIVE, \n",
    "                                    seasonal_periods=mseas) \n",
    "    else:\n",
    "        m_expon = ExponentialSmoothing( trend=ModelMode.MULTIPLICATIVE, \n",
    "                                    damped=False, \n",
    "                                    seasonal=ModelMode.MULTIPLICATIVE, \n",
    "                                    seasonal_periods=mseas) \n",
    "else:\n",
    "    m_expon = ExponentialSmoothing()\n",
    "\n",
    "print(\"model:\", m_expon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: Prophet\n"
     ]
    }
   ],
   "source": [
    "# prepare Prophet forecaster\n",
    "\n",
    "m_prophet = Prophet()    #frequency=mseas)\n",
    "\n",
    "# m_prophet = Prophet(add_seasonalities={'name': 'Mthly','seasonal_periods': 12, 'fourier_order': 1})\n",
    "\n",
    "print(\"model:\", m_prophet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# prepare ARIMA forecaster\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mts\u001b[49m\u001b[38;5;241m.\u001b[39mpd_series())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# get order of first differencing: the higher of KPSS and ADF test results\u001b[39;00m\n\u001b[1;32m      5\u001b[0m n_kpss \u001b[38;5;241m=\u001b[39m pmd\u001b[38;5;241m.\u001b[39marima\u001b[38;5;241m.\u001b[39mndiffs(y, alpha\u001b[38;5;241m=\u001b[39mALPHA, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkpss\u001b[39m\u001b[38;5;124m'\u001b[39m, max_d\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ts' is not defined"
     ]
    }
   ],
   "source": [
    "# prepare ARIMA forecaster\n",
    "\n",
    "y = np.asarray(ts.pd_series())\n",
    "# get order of first differencing: the higher of KPSS and ADF test results\n",
    "n_kpss = pmd.arima.ndiffs(y, alpha=ALPHA, test='kpss', max_d=4)\n",
    "n_adf = pmd.arima.ndiffs(y, alpha=ALPHA, test='adf', max_d=4)\n",
    "n_diff = max(n_adf, n_kpss)\n",
    "min_diff = min(n_adf, n_kpss)\n",
    "\n",
    "# get order of seasonal differencing: the higher of OCSB and CH test results\n",
    "n_ocsb = pmd.arima.OCSBTest(m=mseas).estimate_seasonal_differencing_term(y)\n",
    "n_ch = pmd.arima.CHTest(m=mseas).estimate_seasonal_differencing_term(y)\n",
    "ns_diff = max(n_ocsb, n_ch, is_seasonal * 1)\n",
    "\n",
    "# set up the ARIMA forecaster\n",
    "m_arima = AutoARIMA(\n",
    "    start_p=1, d=min_diff, start_q=1,\n",
    "    max_p=4, max_d=n_diff, max_q=4,\n",
    "    start_P=0, D=ns_diff, start_Q=0, m=max(4,mseas), seasonal=is_seasonal,\n",
    "    max_P=3, max_D=1, max_Q=3,\n",
    "    max_order=5,                       # p+q+p+Q <= max_order\n",
    "    stationary=False, \n",
    "    information_criterion=\"bic\", alpha=ALPHA, \n",
    "    test=\"kpss\", seasonal_test=\"ocsb\",\n",
    "    stepwise=True, \n",
    "    suppress_warnings=True, error_action=\"trace\", trace=TRACE, with_intercept=\"auto\")\n",
    "print(\"model:\", m_arima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m theta \u001b[38;5;129;01min\u001b[39;00m thetas:\n\u001b[1;32m     11\u001b[0m     model \u001b[38;5;241m=\u001b[39m Theta(theta)\n\u001b[0;32m---> 12\u001b[0m     res \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain\u001b[49m)\n\u001b[1;32m     13\u001b[0m     pred_theta \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mlen\u001b[39m(val))\n\u001b[1;32m     14\u001b[0m     res_mape \u001b[38;5;241m=\u001b[39m mape(val, pred_theta)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# prepare Theta forecaster\n",
    "\n",
    "# search space for best theta value: check 100 alternatives\n",
    "thetas = 2 - np.linspace(-10, 10, 100)\n",
    "\n",
    "# initialize search\n",
    "best_mape = float('inf')\n",
    "best_theta = 0\n",
    "# search for best theta among 50 values, as measured by MAPE\n",
    "for theta in thetas:\n",
    "    model = Theta(theta)\n",
    "    res = model.fit(train)\n",
    "    pred_theta = model.predict(len(val))\n",
    "    res_mape = mape(val, pred_theta)\n",
    "\n",
    "    if res_mape < best_mape:\n",
    "        best_mape = res_mape\n",
    "        best_theta = theta\n",
    "\n",
    "m_theta = Theta(best_theta)   # best theta model among 100\n",
    "print(\"model:\", m_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm_expon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# laundry list of forecasters to run\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Comment each individual out to remove if won't run due to data length, etc.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m models \u001b[38;5;241m=\u001b[39m [ \n\u001b[0;32m----> 5\u001b[0m     \u001b[43mm_expon\u001b[49m, \n\u001b[1;32m      6\u001b[0m     m_theta, \n\u001b[1;32m      7\u001b[0m     m_arima,\n\u001b[1;32m      8\u001b[0m     m_naive, \n\u001b[1;32m      9\u001b[0m     m_prophet]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm_expon' is not defined"
     ]
    }
   ],
   "source": [
    "# laundry list of forecasters to run\n",
    "# Comment each individual out to remove if won't run due to data length, etc.\n",
    "\n",
    "models = [ \n",
    "    m_expon, \n",
    "    m_theta, \n",
    "    m_arima,\n",
    "    m_naive, \n",
    "    m_prophet]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# call the forecasters one after the other\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model_predictions \u001b[38;5;241m=\u001b[39m [eval_model(model) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# call the forecasters one after the other\n",
    "\n",
    "model_predictions = [eval_model(model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# RUN the forecasters and tabulate their prediction accuracy and processing time\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_acc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[43mmodel_predictions\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m], orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m df_acc\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(models[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# RUN the forecasters and tabulate their prediction accuracy and processing time\n",
    "\n",
    "df_acc = pd.DataFrame.from_dict(model_predictions[0][1], orient=\"index\")\n",
    "df_acc.columns = [str(models[0])]\n",
    "\n",
    "for i, m in enumerate(models):\n",
    "    if i > 0: \n",
    "        df = pd.DataFrame.from_dict(model_predictions[i][1], orient=\"index\")\n",
    "        df.columns = [str(m)]\n",
    "        df_acc = pd.concat([df_acc, df], axis=1)\n",
    "    i +=1\n",
    "\n",
    "pd.set_option(\"display.precision\",3)\n",
    "#df_acc.style.highlight_min(color=\"lightgreen\", axis=1).highlight_max(color=\"yellow\", axis=1)\n",
    "display(df_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot the forecasts\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pairs \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(\u001b[43mmodels\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)                    \u001b[38;5;66;03m# how many rows of charts\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(pairs, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m pairs))\n\u001b[1;32m      5\u001b[0m ax \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# plot the forecasts\n",
    "\n",
    "pairs = math.ceil(len(models)/2)                    # how many rows of charts\n",
    "fig, ax = plt.subplots(pairs, 2, figsize=(20, 5 * pairs))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i,m in enumerate(models):\n",
    "        ts.plot(label=\"actual\", ax=ax[i])\n",
    "        model_predictions[i][0].plot(label=\"prediction: \"+str(m), ax=ax[i])\n",
    "        \n",
    "        mape_model =  model_predictions[i][1][\"MAPE\"]\n",
    "        time_model =  model_predictions[i][1][\"time\"]\n",
    "        ax[i].set_title(\"\\n\\n\" + str(m)[:30] + \": MAPE {:.1f}%\".format(mape_model) + \" - time {:.1f}sec\".format(time_model))\n",
    "\n",
    "        ax[i].set_xlabel(\"\")\n",
    "        ax[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m act \u001b[38;5;241m=\u001b[39m \u001b[43mval\u001b[49m\n\u001b[1;32m      3\u001b[0m resL \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m resN \u001b[38;5;241m=\u001b[39m {} \n",
      "\u001b[0;31mNameError\u001b[0m: name 'val' is not defined"
     ]
    }
   ],
   "source": [
    "act = val\n",
    "\n",
    "resL = {}\n",
    "resN = {} \n",
    "for i,m in enumerate(models):\n",
    "        pred = model_predictions[i][0]\n",
    "        resid = pred - act\n",
    "        sr = resid.pd_series() \n",
    "\n",
    "        resL[str(m)] = sm.stats.acorr_ljungbox(sr, lags=[5], return_df=False)[1][0]\n",
    "        resN[str(m)] = normaltest(sr)[1]\n",
    "\n",
    "        \n",
    "print(\"\\nLjung-Box test for white-noise residuals: p-value > alpha?\")\n",
    "_ = [print(key,\":\",value) for key,value in resL.items()]\n",
    "\n",
    "print(\"\\ntest for normality of residuals: p-value > alpha?\")\n",
    "_ = [print(key,\":\",value) for key,value in resN.items()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# investigate the residuals in the validation dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m act \u001b[38;5;241m=\u001b[39m \u001b[43mval\u001b[49m\n\u001b[1;32m      4\u001b[0m df_desc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val' is not defined"
     ]
    }
   ],
   "source": [
    "# investigate the residuals in the validation dataset\n",
    "\n",
    "act = val\n",
    "df_desc = pd.DataFrame()\n",
    "\n",
    "for i,m in enumerate(models):\n",
    "        pred = model_predictions[i][0]\n",
    "        resid = pred - act\n",
    "        resid = resid.pd_dataframe()\n",
    "\n",
    "        df_desc = pd.concat([df_desc, resid.describe()], axis=1)\n",
    "\n",
    "        #plot_residuals_analysis(resid);\n",
    "        #plt.title(str(m))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# descriptive statistics of the forecast series\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_desc\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodels\u001b[49m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_desc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# descriptive statistics of the forecast series\n",
    "df_desc.columns = [str(m) for m in models]\n",
    "print(df_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine the individual models in an ensemble forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble forecast evaluation function:\n",
    "# the eval function calls the methods contained in the list 'models'\n",
    "\n",
    "def ensemble_eval(train, val, models):\n",
    "    t_start =  time.perf_counter()\n",
    "    n_train = 12              # use 50 observation to train the ensemble model\n",
    "    print(n_train)\n",
    "    n_val = len(val)            # forecast as many periods as are in the valuation dataset\n",
    "\n",
    "\n",
    "    # compute predictions\n",
    "    # ensemble_model = RegressionEnsembleModel(forecasting_models=models, regression_train_n_points=n_train)\n",
    "    # new Jan 022: RegressionEnsembleModel class no longer accepts as argument the list \"models\" of previously instantiated models\n",
    "    # instead, explicitly list each of the forecast methods\n",
    "    ensemble_model = RegressionEnsembleModel(\n",
    "                                forecasting_models=[    Theta(best_theta), \n",
    "                                                        Prophet(), \n",
    "                                                        AutoARIMA(), \n",
    "                                                        NaiveDrift(),\n",
    "                                                        ExponentialSmoothing(\n",
    "                                                                            trend=ModelMode.MULTIPLICATIVE, \n",
    "                                                                            damped=False, \n",
    "                                                                            seasonal=ModelMode.MULTIPLICATIVE, \n",
    "                                                                            seasonal_periods=mseas)]\n",
    "                                 ,regression_train_n_points=n_train)\n",
    "\n",
    "\n",
    "\n",
    "    ensemble_model.fit(train)\n",
    "    forecast = ensemble_model.predict(n_val)\n",
    "    resid = forecast - val\n",
    "\n",
    "\n",
    "    res_accuracy = accuracy_metrics(val, forecast, resid, t_start)\n",
    "\n",
    "\n",
    "    # plot the ensemble forecast\n",
    "    ts.plot(label=\"actual\")\n",
    "    forecast.plot(label=\"Ensemble forecast\")\n",
    "    plt.title(\"MAPE = {:.2f}%\".format(res_accuracy[\"MAPE\"]))\n",
    "    plt.legend();\n",
    "\n",
    "\n",
    "    results = [forecast, res_accuracy]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# call the ensemble eval function with all 5 forecasters:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# column headers and model selection:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m col_heads \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpon\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mARIMA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProphet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsemble\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m models2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# run the ensemble forecast\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsemble of all 5 forecasters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# call the ensemble eval function with all 5 forecasters:\n",
    "\n",
    "# column headers and model selection:\n",
    "col_heads = [\"Expon\", \"Theta\", \"ARIMA\", \"Naive\", \"Prophet\", \"avg\", \"Ensemble\"]\n",
    "models2 = models\n",
    "\n",
    "# run the ensemble forecast\n",
    "print(\"Ensemble of all 5 forecasters:\")\n",
    "res_ensemble = ensemble_eval(train, val, models2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# collect the accuracy metrics\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df_acc2 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_acc\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      4\u001b[0m df_acc2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_acc2\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m df_acc2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsemble\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(res_ensemble[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_acc' is not defined"
     ]
    }
   ],
   "source": [
    "# collect the accuracy metrics\n",
    "\n",
    "df_acc2 = df_acc.copy()\n",
    "df_acc2[\"avg\"] = df_acc2.mean(axis=1)\n",
    "df_acc2[\"Ensemble\"] = pd.Series(res_ensemble[1])\n",
    "df_acc2.columns = col_heads\n",
    "df_acc2.style.highlight_min(color=\"lightgreen\", axis=1).highlight_max(color=\"yellow\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_ensemble' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#resid = res_ensemble[2]\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m resid \u001b[38;5;241m=\u001b[39m \u001b[43mres_ensemble\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m val\n\u001b[1;32m      4\u001b[0m sr \u001b[38;5;241m=\u001b[39m resid\u001b[38;5;241m.\u001b[39mpd_series()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res_ensemble' is not defined"
     ]
    }
   ],
   "source": [
    "#resid = res_ensemble[2]\n",
    "\n",
    "resid = res_ensemble[0] - val\n",
    "sr = resid.pd_series()\n",
    "#plot_residuals_analysis(resid);\n",
    "#plt.title(\"Ensemble forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resL \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39macorr_ljungbox(\u001b[43msr\u001b[49m, lags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m5\u001b[39m], return_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m resN \u001b[38;5;241m=\u001b[39m normaltest(sr)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLjung-Box test for white-noise residuals: p-value > alpha?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sr' is not defined"
     ]
    }
   ],
   "source": [
    "resL = sm.stats.acorr_ljungbox(sr, lags=[5], return_df=False)[1][0]\n",
    "resN = normaltest(sr)[1]\n",
    "   \n",
    "print(\"\\nLjung-Box test for white-noise residuals: p-value > alpha?\")\n",
    "print(resL)\n",
    "\n",
    "print(\"\\ntest for normality of residuals: p-value > alpha?\")\n",
    "print(resN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot the forecast scenario, and now include the ensemble\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pairs \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(\u001b[43mmodels\u001b[49m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)                    \u001b[38;5;66;03m# how many rows of charts\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(pairs, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m pairs))\n\u001b[1;32m      5\u001b[0m ax \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "# plot the forecast scenario, and now include the ensemble\n",
    "\n",
    "pairs = math.ceil(len(models)/2)                    # how many rows of charts\n",
    "fig, ax = plt.subplots(pairs, 2, figsize=(20, 5 * pairs))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i,m in enumerate(models):\n",
    "        ts.plot(label=\"actual\", ax=ax[i])\n",
    "        model_predictions[i][0].plot(label=\"prediction: \"+str(m)[:30], ax=ax[i])\n",
    "        rmse_model =  model_predictions[i][1][\"RMSE\"]\n",
    "        time_model =  model_predictions[i][1][\"time\"]\n",
    "        ax[i].set_title(\"\\n\\n\" + str(m)[:30] + \": RMSE {:.0f}\".format(rmse_model) + \" - time {:.1f}sec\".format(time_model))\n",
    "        ax[i].set_xlabel(\"\")\n",
    "        ax[i].legend()\n",
    "\n",
    "\n",
    "# add the ensemble:\n",
    "ts.plot(label=\"actual\", ax=ax[i+1])\n",
    "res_ensemble[0].plot(label=\"prediction: Ensemble\", ax=ax[i+1])\n",
    "rmse_model =  res_ensemble[1][\"RMSE\"]\n",
    "time_model =  res_ensemble[1][\"time\"]\n",
    "ax[i+1].set_title(\"\\n\\n Ensemble: RMSE {:.0f}\".format(rmse_model) + \" - time {:.1f}sec\".format(time_model))\n",
    "ax[i+1].set_xlabel(\"\")\n",
    "ax[i+1].legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "199db9df9b9ef57439d34a9130ddcdb1aaf98c03de3e4bfa4df2ecd12126345c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
